{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7a70dab",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692a6e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Modelling\n",
    "from haversine import haversine\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import (StandardScaler, MinMaxScaler)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from scipy.stats import iqr\n",
    "from scipy.stats import scoreatpercentile as pct\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import probplot\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import xgboost\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522ed223",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ff08bf",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a9a21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file, source_type=\"csv\"):\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file : str\n",
    "        Name of the file containing the data.\n",
    "    source_type : str, optional\n",
    "        The file type.\"csv\" or \"excel\" accepted. The default is \"csv\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    file1 : Dataframe\n",
    "        A DataFrame containing the data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    path = \"Data/\" + file\n",
    "\n",
    "    try:\n",
    "\n",
    "        read_f = eval(f'pd.read_{source_type}')\n",
    "        file1 = read_f(path)\n",
    "        return file1\n",
    "\n",
    "    except AttributeError:\n",
    "\n",
    "        print(\"Sorry, your input source_type is not supported. Try 'csv' or 'excel'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b8be3c",
   "metadata": {},
   "source": [
    "## Cleaning and preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92ef5a4",
   "metadata": {},
   "source": [
    "### Standardizing Headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc470221",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def standard_headings(df):\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The DataFrame which headings will be standardized.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "        A DataFrame with standardized headings, i.e. lower case and \" \" replaced by \"_\".\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    heading = df.columns\n",
    "    df.columns = [clabel.lower().replace(\" \", \"_\") for clabel in heading]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71072dc",
   "metadata": {},
   "source": [
    "### Transform yr_renovated and yr_built into one single column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05238be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function (to use the block of code in a pipeline), only usable for this dataset\n",
    "# Combine the renovation year (with many zeros, possibly missing values) and the construction year into one single column\n",
    "\n",
    "def transform_renovated_built(df):\n",
    "\n",
    "    yr_renovated = df[\"yr_renovated\"]\n",
    "    yr_built = df[\"yr_built\"]\n",
    "    yr_ren_built = yr_renovated.where(yr_renovated != 0, other = yr_built)\n",
    "    age=  max(yr_ren_built) - yr_ren_built\n",
    "    df[\"yr_built\"] = age\n",
    "    df = df.rename(columns={\"yr_built\":\"age\"})\n",
    "    df = df.drop(columns=[\"yr_renovated\"])   \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568840b9",
   "metadata": {},
   "source": [
    "### Removing specific rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bfb8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function (to use the block of code in a pipeline), only usable for this dataset\n",
    "# There is one record with 33 rooms that is obviously a typo\n",
    "\n",
    "def remove_rows(df):\n",
    "\n",
    "    df = df[df[\"bedrooms\"] != 33]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b7609a",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a615bf56",
   "metadata": {},
   "source": [
    "### X/y split | Train/test split | Numerical Variables Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413751eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def my_transformations(x, y, test_size=0.2, numerical=True, scaler=\"standard\", \n",
    "                       categorical=True, ordinal_dict=None, nominal_list=[]):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : DataFrame or Series\n",
    "        Estimators\n",
    "    y : DataFrame or Series\n",
    "        Target Variable\n",
    "    test_size : float, optional\n",
    "        Size in fraction (0-1) of the test set. The default is 0.2.\n",
    "    numerical : Boolean, optional\n",
    "        Numerical Flag, True if the output DataFrame should contain numerical variables. \n",
    "        The default is True.\n",
    "    scaler : String, optional\n",
    "        The type of numerical scaling method to be used. Values accepted are \n",
    "        \"standard\" and \"minmax\". The default is \"standard\".\n",
    "    categorical : Boolean, optional\n",
    "        Categorical Flag, True if the output DataFrame should contain categorical variables.\n",
    "        The default is True.\n",
    "    ordinal_dict : Dictionary, optional\n",
    "        Dictionary containing the rules for the ordinal encoding. The default is None.\n",
    "    nominal_list : List, optional\n",
    "        List containing the columns that require one-hot encoding . The default is [].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing two records, \"train\" and \"test\".\n",
    "        These two are lists containing the transformed train and test set:\n",
    "            - Scaled Numerical columns\n",
    "            - Ordinal Encoded columns\n",
    "            - One-Hot Encoded columns\n",
    "            - Target Variable\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Train/Test Split\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=0)\n",
    "\n",
    "    x_train = x_train.reset_index(drop=True)\n",
    "    x_test = x_test.reset_index(drop=True)\n",
    "\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "    # Numerical variables - assign and scale them\n",
    "\n",
    "    if numerical:\n",
    "\n",
    "        x_train_num = x_train._get_numeric_data()\n",
    "        x_test_num = x_test._get_numeric_data()\n",
    "\n",
    "        if scaler is not None:\n",
    "\n",
    "            if scaler == \"minmax\":\n",
    "\n",
    "                scaler = MinMaxScaler()\n",
    "\n",
    "            elif scaler == \"standard\":\n",
    "\n",
    "                scaler = StandardScaler()\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(\"The input scaling method is not supported. Please use 'standard' or 'minmax'.\")\n",
    "                return\n",
    "\n",
    "            scaler.fit(x_train_num)\n",
    "            output_train_num = pd.DataFrame(scaler.transform(x_train_num), columns=x_train_num.columns)\n",
    "            output_test_num = pd.DataFrame(scaler.transform(x_test_num), columns=x_test_num.columns)\n",
    "\n",
    "        else:\n",
    "\n",
    "            output_train_num = x_train_num\n",
    "            output_test_num = x_test_num\n",
    "    else:\n",
    "\n",
    "        output_train_num = \"None\"\n",
    "        output_test_num = \"None\"\n",
    "\n",
    "    # Categorical variables - assign and encode them\n",
    "\n",
    "    if categorical:\n",
    "\n",
    "        x_train_cat = x_train.select_dtypes([\"category\", \"object\"])\n",
    "        x_test_cat = x_test.select_dtypes([\"category\", \"object\"])\n",
    "\n",
    "        # Encoding - Ordinals\n",
    "\n",
    "        if ordinal_dict is not None:\n",
    "\n",
    "            x_train_cat_ord = x_train_cat[list(ordinal_dict.keys())]\n",
    "            x_test_cat_ord = x_test_cat[list(ordinal_dict.keys())]\n",
    "\n",
    "            categories = [t[1] for t in list(ordinal_dict.items())]\n",
    "\n",
    "            ordinal_encoder = OrdinalEncoder(categories=categories)\n",
    "            x_train_cat[x_train_cat_ord.columns] = pd.DataFrame(ordinal_encoder.fit_transform(x_train_cat_ord)\n",
    "                                                                , columns=x_train_cat_ord.columns)\n",
    "            output_train_cat_ord = x_train_cat.drop(nominal_list, axis=1)\n",
    "            x_test_cat[x_test_cat_ord.columns] = pd.DataFrame(ordinal_encoder.fit_transform(x_test_cat_ord)\n",
    "                                                              , columns=x_test_cat_ord.columns)\n",
    "            output_test_cat_ord = x_test_cat.drop(nominal_list, axis=1)\n",
    "\n",
    "        else:\n",
    "\n",
    "            output_train_cat_ord = x_train_cat\n",
    "            output_test_cat_ord = x_test_cat\n",
    "\n",
    "        # Encoding - Nominals\n",
    "\n",
    "        if len(nominal_list) != 0:\n",
    "\n",
    "            output_train_cat_nom = pd.get_dummies(x_train_cat.loc[:, nominal_list], drop_first=True)\n",
    "            output_test_cat_nom = pd.get_dummies(x_test_cat.loc[:, nominal_list], drop_first=True)\n",
    "\n",
    "        else:\n",
    "\n",
    "            output_train_cat_nom = \"None\"\n",
    "            output_test_cat_nom = \"None\"\n",
    "\n",
    "    else:\n",
    "\n",
    "        output_train_cat_ord = \"None\"\n",
    "        output_train_cat_nom = \"None\"\n",
    "        output_test_cat_ord = \"None\"\n",
    "        output_test_cat_nom = \"None\"\n",
    "\n",
    "    return {\"train\": [output_train_num, output_train_cat_ord, output_train_cat_nom, y_train],\n",
    "            \"test\": [output_test_num, output_test_cat_ord, output_test_cat_nom, y_test]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc60a58",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d8a860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_normalization(df):\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame or Series\n",
    "        Target Dataframe to be nomalized.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    transformer : Sklearn Transformet Object\n",
    "        Box-Cox Transformer Object.\n",
    "    power_norm : DataFrame or Series\n",
    "        Target DataFrame or Series Normalized using a box-cox transformation.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    transformer = PowerTransformer(method=\"box-cox\").fit(df.to_numpy().reshape(-1, 1))\n",
    "    power_norm = transformer.transform(df.to_numpy().reshape(-1, 1))\n",
    "    power_norm = pd.DataFrame(power_norm)[0]\n",
    "\n",
    "    return transformer, power_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db81cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function (to use the block of code in a pipeline), only usable for this dataset\n",
    "# Normalization Tranform of price and sqft_living\n",
    "\n",
    "def normalizations(df):\n",
    "\n",
    "    df[\"price\"] = np.log(df[\"price\"])\n",
    "    df[\"sqft_living\"] = var_normalization(df[\"sqft_living\"])[1]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a66fadd",
   "metadata": {},
   "source": [
    "### Location Engineered Feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12491256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function (to use the block of code in a pipeline), only usable for this dataset\n",
    "# Engineered Feature to treat the location data as the distance of each house to two hot spots, bellevue and Seattle downtown\n",
    "\n",
    "def dist_to(df):\n",
    "\n",
    "    latlong = df.loc[:, ['lat', 'long']]\n",
    "    latlong_tuple = list(zip(latlong.loc[:, 'lat'], latlong.loc[:, 'long']))\n",
    "    latlong[\"lat_long\"] = latlong_tuple\n",
    "\n",
    "    dist_to_seattle = latlong.loc[:, 'lat_long'].apply(haversine, point2=(47.609395, -122.336283))\n",
    "    dist_to_bellevue = latlong.loc[:, 'lat_long'].apply(haversine, point2=(47.616492, -122.188985))\n",
    "    \n",
    "    df.loc[:, 'dist_to_seattle'] = dist_to_seattle\n",
    "    df.loc[:, 'dist_to_bellevue'] = dist_to_bellevue\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb02ca2",
   "metadata": {},
   "source": [
    "## Regression Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648d02ca",
   "metadata": {},
   "source": [
    "### Performance Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0840b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_adjusted(x, y, y_pred, r2=None):\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : DataFrame or Serie\n",
    "        Estimators\n",
    "    y : DataFrame or Serie\n",
    "        Observed Target Variable.\n",
    "    y_pred : DataFrame or Serie\n",
    "        Predicted Target Variable.\n",
    "    r2 : Float, optional\n",
    "        R2 value previously calculated. The default is None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    r2_adj : Float\n",
    "        Adjusted R2 value.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if r2 is None:\n",
    "\n",
    "        r2 = r2_score(y, y_pred)\n",
    "\n",
    "\n",
    "    r2_adj = 1 - (1 - r2) * (len(y) - 1) / (len(y) - x.shape[1] - 1)\n",
    "\n",
    "    return r2_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe097149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_regression(y_train, y_test, y_pred_train, y_pred_test, x_train, x_test):\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_train : DataFrame or Series\n",
    "        Observed Target Variable (train set).\n",
    "    y_test : DataFrame or Series\n",
    "        Observed Target Variable (test set).\n",
    "    y_pred_train : DataFrame or Series\n",
    "        Predicted Target Variable (train set).\n",
    "    y_pred_test : DataFrame or Series\n",
    "        Predicted Target Variable (test set).\n",
    "    x_train : DataFrame or Series\n",
    "        Estimators (train set).\n",
    "    x_test : DataFrame or Series\n",
    "        Estimators (test set).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    performance : DataFrame\n",
    "        Dataframe containing the performance metrics (MAE, MSE, RMSE, R2 and R2 adj).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    performance = pd.DataFrame({'error_metric': ['mae', 'mse', 'rmse', 'r2', 'r2_adj'],\n",
    "                                'train': [mae(y_train, y_pred_train),\n",
    "                                          mse(y_train, y_pred_train),\n",
    "                                          mse(y_train, y_pred_train, squared=False),\n",
    "                                          r2_score(y_train, y_pred_train),\n",
    "                                          r2_adjusted(x_train, y_train, y_pred_train)],\n",
    "                                'test':  [mae(y_test, y_pred_test),\n",
    "                                          mse(y_test, y_pred_test),\n",
    "                                          mse(y_test, y_pred_test, squared=False),\n",
    "                                          r2_score(y_test, y_pred_test),\n",
    "                                          r2_adjusted(x_test, y_test, y_pred_test)]})\n",
    "\n",
    "    performance = performance.set_index(\"error_metric\")\n",
    "\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d46821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_plots(pred_data, set_id, fig_size=(15, 5)):\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pred_data : Dictionary\n",
    "        Contains the observed and the predicted target variable.\n",
    "    set_id : String\n",
    "        \"train\" or \"test\".\n",
    "    fig_size : Tuple, optional\n",
    "        Figure size. The default is (15, 5).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    y = pred_data[\"y\"]\n",
    "    y_pred = pred_data[\"y_pred\"]\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=fig_size)\n",
    "\n",
    "    sns.regplot(x=\"y\", y=\"y_pred\", data=pred_data, scatter_kws={\"color\": \"red\"}, line_kws={\"color\": \"black\"}, ax=axs[0])\n",
    "    sns.histplot(y - y_pred, kde=True, ax=axs[1])\n",
    "    axs[2].plot(y_pred, y - y_pred, \"o\")\n",
    "    axs[2].plot(y_pred, np.zeros(len(y_pred)), linestyle='dashed')\n",
    "\n",
    "    axs[0].set_title(f\"{set_id}\".capitalize() + \" Set - Observed VS Predicted\")\n",
    "    axs[1].set_title(f\"{set_id}\".capitalize() + \" Set - Histogram of the Residuals\")\n",
    "    axs[2].set_title(\"Residuals by Predicted\")\n",
    "\n",
    "    axs[1].set_xlabel(f\"y_{set_id}\" + \" - y_pred\")\n",
    "    axs[2].set_xlabel(\"Predicted\")\n",
    "    axs[2].set_ylabel(\"Residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c5976f",
   "metadata": {},
   "source": [
    "### Flexible Method Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27971c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(x_train, y_train, x_test, y_test, model, cv=10, verbose=True, plot=True, y_transformer=None):\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_train : DataFrame or Series\n",
    "        Estimators (train set).\n",
    "    y_train : DataFrame or Series\n",
    "        Observed Target Variable (train set).\n",
    "    x_test : DataFrame or Series\n",
    "        Estimators (test set).\n",
    "    y_test : DataFrame or Series\n",
    "        Observed Target Variable (test set).\n",
    "    model : Sklearn model object\n",
    "        Model to be used for the prediction\n",
    "    cv : Integer, optional\n",
    "        Number of cross validation folds. The default is 10.\n",
    "    verbose : Boolean, optional\n",
    "        The default is True.\n",
    "    plot : Boolean, optional\n",
    "        The default is True.\n",
    "    y_transformer : String or sklearn transformer object, optional\n",
    "        The normalization transformer used on the target variable, if any. The default is None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Results of the regression.\n",
    "        - Fitted Model\n",
    "        - Cross Validation Scores\n",
    "        - Prediction results\n",
    "        - Performance Metrics\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Model cross validation\n",
    "    val_scores = cross_val_score(model, x_train, y_train, cv=cv)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(x_train)\n",
    "    y_pred_test = model.predict(x_test)\n",
    "\n",
    "    if y_transformer is not None:\n",
    "\n",
    "        if y_transformer != \"log\":\n",
    "\n",
    "            y_train = pd.DataFrame(y_transformer.inverse_transform(np.array(y_train).reshape(-1, 1)))[0]\n",
    "            y_pred_train = pd.DataFrame(y_transformer.inverse_transform(y_pred_train.reshape(-1, 1)))[0]\n",
    "\n",
    "            y_test = pd.DataFrame(y_transformer.inverse_transform(np.array(y_test).reshape(-1, 1)))[0]\n",
    "            y_pred_test = pd.DataFrame(y_transformer.inverse_transform(y_pred_test.reshape(-1, 1)))[0]\n",
    "\n",
    "        elif y_transformer == \"log\":\n",
    "\n",
    "            y_train = np.exp(y_train)\n",
    "            y_pred_train = np.exp(y_pred_train)\n",
    "\n",
    "            y_test = np.exp(y_test)\n",
    "            y_pred_test = np.exp(y_pred_test)\n",
    "\n",
    "    prediction_results = pd.DataFrame({\"train\": {\"y\": y_train, \"y_pred\": y_pred_train},\n",
    "                                       \"test\": {\"y\": y_test, \"y_pred\": y_pred_test}})\n",
    "\n",
    "    # Build the performance df\n",
    "    performance_metrics = pd.DataFrame({\"error_metric\": [f'val_mean_score (k={cv})', f'val_std (k={cv})'],\n",
    "                                        \"train\": [round(np.mean(val_scores), 3), round(np.std(val_scores), 3)],\n",
    "                                        \"test\": [\"-\", \"-\"]})\n",
    "    performance_metrics = performance_metrics.set_index(\"error_metric\")\n",
    "\n",
    "    # Performance Evaluation\n",
    "    error_metrics = perf_regression(y_train, y_test, y_pred_train, y_pred_test, x_train, x_test)\n",
    "    performance_metrics = pd.concat([performance_metrics, error_metrics], axis=0)\n",
    "    performance_metrics[\"train\"] = performance_metrics[\"train\"].astype(\"object\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"-----------------\")\n",
    "        print(f'The model score using K-fold cross validation (k={cv}) is {round(np.mean(val_scores), 3)} '\n",
    "              f'with a standard deviation of {round(np.std(val_scores), 3)}')\n",
    "        print(\"-----------------\")\n",
    "        print(\"The performance metrics of the model\")\n",
    "        display(performance_metrics)\n",
    "        print(\"-----------------\")\n",
    "\n",
    "    if plot:\n",
    "        \n",
    "        regression_plots(prediction_results[\"train\"], set_id=\"train\", fig_size=(15, 5))\n",
    "        regression_plots(prediction_results[\"test\"], set_id=\"test\", fig_size=(15, 5))\n",
    "\n",
    "    return {\"model\": model, \"val_scores\": val_scores, \"prediction_results\": prediction_results,\n",
    "            \"performance_metrics\": performance_metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf07e8a",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037aec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot a correlation map\n",
    "\n",
    "def my_correlation_heatmap(df, figsize = (16,16)):\n",
    "    \"\"\"\n",
    "    A function to plot a correlation heatmap from a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The target DataFrame\n",
    "    figsize : Tuple, optional\n",
    "        Tuple seting the figure size. The default is (16,16).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # correlation matrix\n",
    "\n",
    "    correlation_matrix = df.corr()\n",
    "\n",
    "    # create figure and axes\n",
    "    fig, ax = plt.subplots(figsize = figsize)\n",
    "\n",
    "    # set title\n",
    "    ax.set_title('Correlation Heatmap', fontweight='bold')\n",
    "\n",
    "\n",
    "    sns.heatmap(correlation_matrix,  # the data for the heatmap\n",
    "                annot=True,  # show the actual values of correlation\n",
    "                cmap='seismic',  # provide the 'seismic' colormap\n",
    "                center=0,  # specify the value at which to center the colormap\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c57ec6",
   "metadata": {},
   "source": [
    "## Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875e9e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to store in a dictionary the number of nan values per column\n",
    "\n",
    "def nan_counter(df):\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The target DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    remaining_nan : Dictionary\n",
    "        A dictionary with the following structure. Key is the df column, value \n",
    "        is the number of null values in the corresponding column.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    remaining_nan = {}\n",
    "\n",
    "    for column in df.columns:\n",
    "\n",
    "        remaining_nan[column] = df[column][df[column].isna() == True].size\n",
    "\n",
    "    return remaining_nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bd89f4",
   "metadata": {},
   "source": [
    "# Initial Data Exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce019d82",
   "metadata": {},
   "source": [
    "Let us load the data using the user defined function \"load_data()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6739d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df = load_data(\"Data_MidTerm_Project_Real_State_Regression.xls\", source_type=\"excel\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5c122a",
   "metadata": {},
   "source": [
    "We standarize the headings of the columns using user defined function \"standard_headings()\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be72b966",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df = standard_headings(hp_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d02429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f800a555",
   "metadata": {},
   "source": [
    "We do a first check of the data using methods \"info()\" and \"describe().\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bd0c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8554faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a3c044",
   "metadata": {},
   "source": [
    "We check also the DataFrame shape. We have 21,597 records and 21 features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e98e978",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df_shape = hp_df.shape\n",
    "hp_df_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eddb38",
   "metadata": {},
   "source": [
    "We will also check the number of null values in all columns using the user defined function \"nan_counter().\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ce787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_counter(hp_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98e0d67",
   "metadata": {},
   "source": [
    "We drop duplicates records and check the new df size. There are no duplicates records.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd45e7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df.drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85465cf",
   "metadata": {},
   "source": [
    "## Price Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2ad0cb",
   "metadata": {},
   "source": [
    "The house price is our dependent variable. Let us create a dedicated variable for easy access and some price analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b369a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "price = hp_df[\"price\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdf4525",
   "metadata": {},
   "source": [
    "### Description and distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a68a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics summary\n",
    "price.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5105106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "\n",
    "# create figure and axes\n",
    "fig, ax = plt.subplots(figsize = (10,6))\n",
    "\n",
    "sns.distplot(price);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6366d5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness and Kurtosis\n",
    "print(f\"Skewness: {price.skew()}\")\n",
    "print(f\"Kurtosis: {price.kurt()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f28281",
   "metadata": {},
   "source": [
    "The price is not following a normal distribution, with a high positive skewness and Kurtosis as a result of the very 'long' right tail. We will have therefore a significant number of 'outliers' in a boxplot. We will need to figure out if the outliers should be removed or if they are required to describe the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed458c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Boxplot\n",
    "\n",
    "# create figure and axes\n",
    "fig, ax = plt.subplots(figsize = (10,6))\n",
    "\n",
    "sns.boxplot(data=price, x=price);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4c4844",
   "metadata": {},
   "source": [
    "### Feature Correlations with Price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9f886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_correlation_heatmap(hp_df, figsize=(16,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37683868",
   "metadata": {},
   "source": [
    "Let us plot a reduced version of the heatmap only with the 10 more correlated features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_corr_sorted = hp_df.corr()[\"price\"].sort_values(ascending=False)\n",
    "top_10_price_corr = list(price_corr_sorted.index[0:11])\n",
    "top_10_price_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7113f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_correlation_heatmap(hp_df[top_10_price_corr], figsize=(8,8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f639f065",
   "metadata": {},
   "source": [
    "The predictors with the highest correlation with the price are sqft_living (living area) and grade (building quality). Let us take a deeper look into these two.\n",
    "\n",
    "It is also interesting the high correlation between sqft_linving and the other features related with area. We will drop all except sqft_living.\n",
    "\n",
    "For the price we can use a simple scatter plot. It is obvious that there is a relatively strong linear correlation between price an sqft_living. We can also see that the houses with very high prices (outliers) have in general also very high living areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da9e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot with sqft_living\n",
    "var = 'sqft_living'\n",
    "data = pd.concat([price, hp_df[var]], axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8,8))\n",
    "sns.regplot(x=var, y=\"price\", data=data, scatter_kws={\"color\": \"red\"}, line_kws={\"color\": \"black\"});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f149892",
   "metadata": {},
   "source": [
    "'Grade' is a categorical variable. A boxplot is more suited therefore than a scatter plot. We can observe an interesting exponential relation between price an grade. Again the very expensive houses have in general very high grades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce131fc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#box plot grade/saleprice\n",
    "var = 'grade'\n",
    "data = pd.concat([price, hp_df[var]], axis=1)\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "fig = sns.boxplot(x=var, y=\"price\", data=data);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0500278b",
   "metadata": {},
   "source": [
    "It is intuitive than grade an condition are kind representing the same information. Let us plot boxplots of the condtion. It is clear (also in the correlation heatmap) that condition has not a significan relation with the price. We will drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdecd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#box plot condition/saleprice\n",
    "var = 'condition'\n",
    "data = pd.concat([price, hp_df[var]], axis=1)\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "fig = sns.boxplot(x=var, y=\"price\", data=data);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c14183",
   "metadata": {},
   "source": [
    "We plot also boxplots for the number of bedrooms and bathrooms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631d6652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#box plot n bathrooms/saleprice\n",
    "var = 'bathrooms'\n",
    "data = pd.concat([price, hp_df[var]], axis=1)\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "fig = sns.boxplot(x=var, y=\"price\", data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9c64eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#box plot n bedrooms/saleprice\n",
    "var = 'bedrooms'\n",
    "data = pd.concat([price, hp_df[var]], axis=1)\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "fig = sns.boxplot(x=var, y=\"price\", data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8138f3d0",
   "metadata": {},
   "source": [
    "Another feature that could relevant is the yr_renovated (renovation year). Unfourtunately this features has many records with value 0. We will ned to deal with this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a241eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Records with yr_renovated = 0\n",
    "\n",
    "yr_renovated_n_0 = hp_df.yr_renovated.value_counts(dropna=False).sort_index()\n",
    "\n",
    "print(f'The number of record with availabe info regarding the renovation year is {hp_df_shape[0] - yr_renovated_n_0[0]}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fbb857",
   "metadata": {},
   "source": [
    "# Dropping features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e6f3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = [\"id\", \"date\",\"sqft_lot\", \"sqft_above\", \"sqft_basement\", \"sqft_lot15\", \"condition\"]\n",
    "\n",
    "# id and date are useless for our analysis.\n",
    "# sqft_lot, sqft_above, sqft_basement and sqft_lot15 are highly correlated with sqft_living and therefore redundant.\n",
    "# the information contained in condition is already in grade and its correlation with price is very low. It can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94945550",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df = hp_df.drop(columns = drop_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50772677",
   "metadata": {},
   "source": [
    "# Data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b08d856",
   "metadata": {},
   "source": [
    "## Building Age treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29428c49",
   "metadata": {},
   "source": [
    "### Combine yr_built and yr_renovated into one single feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23247f79",
   "metadata": {},
   "source": [
    "What about if we combine the renovation year (with many zeros, possibly missing values) and the construction year into one single column. We will replace the construction year by the renovation year, for those records (very few) whit information regarding the renovation. This basically assumes that a building renovation equals to a new construction in terms of prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0057777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yr_built = hp_df[\"yr_built\"].copy()\n",
    "yr_renovated = hp_df[\"yr_renovated\"].copy()\n",
    "yr_ren_built = yr_renovated.where(yr_renovated != 0, other = yr_built)\n",
    "yr_ren_built"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3df859",
   "metadata": {},
   "source": [
    "Let us now convert this new feature into a variable representing the age.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2550a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(yr_ren_built)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121adde1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "age =  max(yr_ren_built) - yr_ren_built\n",
    "age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1819f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df[\"age\"] = age\n",
    "hp_df = hp_df.drop(columns=[\"yr_renovated\", \"yr_built\"])\n",
    "hp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c8833d",
   "metadata": {},
   "source": [
    "## Dropping row with 33 bedrooms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb07ac",
   "metadata": {},
   "source": [
    "There is one record with 33 rooms that is obviously a typo. We will remove this row.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd92f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df = hp_df[hp_df[\"bedrooms\"] != 33]\n",
    "hp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c07724a",
   "metadata": {},
   "source": [
    "## Location Variables - distto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a43949",
   "metadata": {},
   "source": [
    "Before starting the analysis, it is reasonable to expect that the location of the house has a significant impact on the house price. However the location estimators in the dataset (zipcode, long and lat) does not show a high correlation with the price. Can we engineer a new feature that better represent the house location?\n",
    "\n",
    "In this section we will engineer a feature representing the disctance of each of the houses to a specific point, in our case to Seattle and two Bellevue. These two locations are hot spots, areas where the house prices are very high. To calculate the distance we will use the haversine function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dbe7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "latlong = hp_df.loc[:, ['lat', 'long']]\n",
    "latlong_tuple = list(zip(latlong.loc[:, 'lat'], latlong.loc[:, 'long']))\n",
    "latlong[\"lat_long\"] = latlong_tuple\n",
    "latlong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41889fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_to_seattle = latlong.loc[:, 'lat_long'].apply(haversine, point2=(47.609395, -122.336283))\n",
    "\n",
    "dist_to_bellevue = latlong.loc[:, 'lat_long'].apply(haversine, point2=(47.616492, -122.188985))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a767493",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df.loc[:, 'dist_to_seattle'] = dist_to_seattle\n",
    "hp_df.loc[:, 'dist_to_bellevue'] = dist_to_bellevue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116485b6",
   "metadata": {},
   "source": [
    "The correlation heatmap shows a better correlation with price of these new two features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c7e751",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_correlation_heatmap(hp_df, figsize=(16,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6afb72e",
   "metadata": {},
   "source": [
    "## Normalization - Price\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2b5e81",
   "metadata": {},
   "source": [
    "Let us analyse how normal is the price distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639e4c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure and axes\n",
    "fig, axs = plt.subplots(1,2,figsize = (12,6))\n",
    "\n",
    "sns.distplot(price,fit=norm, ax=axs[0])\n",
    "result = probplot(price, plot=plt);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97f31c2",
   "metadata": {},
   "source": [
    "It is clear there is some room for improvement. Let us normalize the price. A logarithmic transformation usually works fine with distribution with long one sided tails.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33674391",
   "metadata": {},
   "source": [
    "### Logarithmic Transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb7ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_log_norm = np.log(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc34409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure and axes\n",
    "fig, axs = plt.subplots(1,2,figsize = (12,6))\n",
    "\n",
    "sns.distplot(price_log_norm,fit=norm, ax=axs[0])\n",
    "result = probplot(price_log_norm, plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55810c26",
   "metadata": {},
   "source": [
    "Now the price follows an almost perfect normal distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a004d886",
   "metadata": {},
   "source": [
    "## Normalization - sqft_living\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467fd3e9",
   "metadata": {},
   "source": [
    "We will do the same with our main numerical estimator, sqft_living.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c30502",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = \"sqft_living\"\n",
    "\n",
    "# create figure and axes\n",
    "fig, axs = plt.subplots(1,2,figsize = (16,8))\n",
    "\n",
    "sns.distplot(hp_df[var],fit=norm, ax=axs[0])\n",
    "result = probplot(hp_df[var], plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00de1415",
   "metadata": {},
   "source": [
    "### Power Transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ec2347",
   "metadata": {},
   "source": [
    "This time we will be using a box-cox transformation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8181cb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqft_living_transformer = PowerTransformer(method=\"box-cox\").fit(hp_df[var].to_numpy().reshape(-1,1))\n",
    "sqft_living_power_norm = sqft_living_transformer.transform(hp_df[var].to_numpy().reshape(-1,1))\n",
    "sqft_living_power_norm = pd.DataFrame(sqft_living_power_norm)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec20ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure and axes\n",
    "fig, axs = plt.subplots(1,2,figsize = (16,8))\n",
    "\n",
    "sns.distplot(sqft_living_power_norm,fit=norm, ax=axs[0])\n",
    "result = probplot(sqft_living_power_norm, plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a2660d",
   "metadata": {},
   "source": [
    "The resultant distribution again follows very closely the normal one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dd1a8b",
   "metadata": {},
   "source": [
    "# Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779924a6",
   "metadata": {},
   "source": [
    "## Model 1 - Simple Linear Regression (sqft_living VS Price)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c277597",
   "metadata": {},
   "source": [
    "This first model is a very simple one: a linear regression using only the main numerical estimator, sqft_living.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b99a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "\n",
    "hp_df = (load_data(\"Data_MidTerm_Project_Real_State_Regression.xls\", source_type=\"excel\")\n",
    "         .pipe(standard_headings)\n",
    "         .pipe(remove_rows)\n",
    "         .reset_index(drop=True)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f77955",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = hp_df[\"sqft_living\"]\n",
    "y = hp_df[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9664b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_transformed = my_transformations(x, y, test_size=0.2, numerical=True, scaler=None, categorical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913465c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.DataFrame(model1_transformed[\"train\"][0])\n",
    "y_train = model1_transformed[\"train\"][3]\n",
    "\n",
    "x_test = pd.DataFrame(model1_transformed[\"test\"][0])\n",
    "y_test = model1_transformed[\"test\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90254c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_output = regression(x_train, y_train, x_test, y_test, LinearRegression(), cv=10, verbose=True, plot=True, y_transformer=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ebedeb",
   "metadata": {},
   "source": [
    "## Model 2 - Multiple Linear Regression (all variables) - with statsmodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef20137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs for the Pipeline Controller\n",
    "\n",
    "drop_columns = [\"id\", \"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b96d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Control\n",
    "\n",
    "hp_df = (load_data(\"Data_MidTerm_Project_Real_State_Regression.xls\", source_type=\"excel\")\n",
    "         .pipe(standard_headings)\n",
    "         .drop(drop_columns, axis=1)\n",
    "         .pipe(remove_rows)\n",
    "         .reset_index(drop=True)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74a29fc",
   "metadata": {},
   "source": [
    "Model 2 considers all features (except id and date, which are irrelevant), no transformations and a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f06df0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = hp_df[\"price\"]\n",
    "x = hp_df.drop([\"price\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74f2b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_transformed = my_transformations(x, y, test_size=0.2, numerical=True, scaler=None, categorical=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19fb776",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.concat([model2_transformed[\"train\"][0], model2_transformed[\"train\"][1]], axis=1)\n",
    "y_train = model2_transformed[\"train\"][3]\n",
    "\n",
    "x_test = pd.concat([model2_transformed[\"test\"][0], model2_transformed[\"test\"][1]], axis=1)\n",
    "y_test = model2_transformed[\"test\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b69563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary from statsmodels\n",
    "\n",
    "x_train_const = sm.add_constant(x_train)  \n",
    "\n",
    "model = sm.OLS(y_train, x_train_const).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae4f165",
   "metadata": {},
   "source": [
    "The hypothesis testing shows that sqft_lot and floors don't have an impact on the house price. The t value also shows that grade and sqft_living are the two more important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2432a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_output = regression(x_train, y_train, x_test, y_test, LinearRegression(), cv=10, verbose=True, plot=True, y_transformer=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622d42e7",
   "metadata": {},
   "source": [
    "## Model 3 - Multiple Linear Regression - Reduced number of variables and Transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd1b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs for the Pipeline Controller\n",
    "\n",
    "drop_columns = [\"id\", \"date\",\"sqft_lot\", \"sqft_above\", \"sqft_basement\", \"sqft_lot15\", \"condition\", \"floors\", \"sqft_living15\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d56022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pipeline Control\n",
    "\n",
    "hp_df = (load_data(\"Data_MidTerm_Project_Real_State_Regression.xls\", source_type=\"excel\")\n",
    "                   .pipe(standard_headings)\n",
    "                   .drop(drop_columns, axis=1)\n",
    "                   .pipe(transform_renovated_built)  # Combination of yr_built and yr_renovated into one single feature\n",
    "                   .pipe(remove_rows)\n",
    "                   .reset_index(drop=True)\n",
    "                   .pipe(normalizations)     # Price and sqft_living Normalization\n",
    "                   .pipe(dist_to)            # Location Enigeneered Feature\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5118c3",
   "metadata": {},
   "source": [
    "Model 3 considers a reduced set of features, the discussed transformations and a linear regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfdc49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = hp_df[\"price\"]\n",
    "x = hp_df.drop([\"price\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dced551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_transformed = my_transformations(x, y, test_size=0.2, numerical=True, scaler=None, categorical=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c77564",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.concat([model3_transformed[\"train\"][0], model3_transformed[\"train\"][1]], axis=1)\n",
    "y_train = model3_transformed[\"train\"][3]\n",
    "\n",
    "x_test = pd.concat([model3_transformed[\"test\"][0], model3_transformed[\"test\"][1]], axis=1)\n",
    "y_test = model3_transformed[\"test\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b231980",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_output = regression(x_train, y_train, x_test, y_test, LinearRegression(), cv=10, verbose=True, plot=True, y_transformer=\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bc8663",
   "metadata": {},
   "source": [
    "## Model 4 - Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af51409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs for the Pipeline Controller\n",
    "\n",
    "drop_columns = [\"id\", \"date\",\"sqft_lot\", \"sqft_above\", \"sqft_basement\", \"sqft_lot15\", \"condition\", \"floors\", \"sqft_living15\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bc1184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Control\n",
    "\n",
    "hp_df = (load_data(\"Data_MidTerm_Project_Real_State_Regression.xls\", source_type=\"excel\")\n",
    "                   .pipe(standard_headings)\n",
    "                   .drop(drop_columns, axis=1)\n",
    "                   .pipe(transform_renovated_built)  # Combination of yr_built and yr_renovated into one single feature\n",
    "                   .pipe(remove_rows)\n",
    "                   .reset_index(drop=True)\n",
    "                   .pipe(normalizations)     # Price and sqft_living Normalization\n",
    "                   .pipe(dist_to)            # Location Enigeneered Feature\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e33d91",
   "metadata": {},
   "source": [
    "Model 4 considers a reduced set of features, the discussed transformations and a ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcb5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = hp_df[\"price\"]\n",
    "x = hp_df.drop([\"price\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c03b9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4_transformed = my_transformations(x, y, test_size=0.2, numerical=True, scaler=None, categorical=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e103516",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.concat([model4_transformed[\"train\"][0], model4_transformed[\"train\"][1]], axis=1)\n",
    "y_train = model4_transformed[\"train\"][3]\n",
    "\n",
    "x_test = pd.concat([model4_transformed[\"test\"][0], model4_transformed[\"test\"][1]], axis=1)\n",
    "y_test = model4_transformed[\"test\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12029c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4_output = regression(x_train, y_train, x_test, y_test, Ridge(alpha=100), cv=10, verbose=True, plot=True, y_transformer=\"log\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f097d3e",
   "metadata": {},
   "source": [
    "Ridge is not bringing any improvement. Kind of expected, the model is not too complex, on the contrary is lacking complexity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411aff05",
   "metadata": {},
   "source": [
    "## Model 5 - Lasso Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d9e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs for the Pipeline Controller\n",
    "\n",
    "drop_columns = [\"id\", \"date\",\"sqft_lot\", \"sqft_above\", \"sqft_basement\", \"sqft_lot15\", \"condition\", \"floors\", \"sqft_living15\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e9ca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Control\n",
    "\n",
    "hp_df = (load_data(\"Data_MidTerm_Project_Real_State_Regression.xls\", source_type=\"excel\")\n",
    "                   .pipe(standard_headings)\n",
    "                   .drop(drop_columns, axis=1)\n",
    "                   .pipe(transform_renovated_built)  # Combination of yr_built and yr_renovated into one single feature\n",
    "                   .pipe(remove_rows)\n",
    "                   .reset_index(drop=True)\n",
    "                   .pipe(normalizations)     # Price and sqft_living Normalization\n",
    "                   .pipe(dist_to)            # Location Enigeneered Feature\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09caa96",
   "metadata": {},
   "source": [
    "Model 5 considers a reduced set of features, the discussed transformations and a Lasso regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6b0b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = hp_df[\"price\"]\n",
    "x = hp_df.drop([\"price\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9cf52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5_transformed = my_transformations(x, y, test_size=0.2, numerical=True, scaler=None, categorical=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284e43be",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.concat([model5_transformed[\"train\"][0], model5_transformed[\"train\"][1]], axis=1)\n",
    "y_train = model5_transformed[\"train\"][3]\n",
    "\n",
    "x_test = pd.concat([model5_transformed[\"test\"][0], model5_transformed[\"test\"][1]], axis=1)\n",
    "y_test = model5_transformed[\"test\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4157eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5_output = regression(x_train, y_train, x_test, y_test, Lasso(alpha=1), cv=10, verbose=True, plot=True, y_transformer=\"log\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b381306",
   "metadata": {},
   "source": [
    "Lasso is not working properly, unless we use ver low alphas. Makes sense, we have very few features and Lasso even reducing some more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516deeb8",
   "metadata": {},
   "source": [
    "## Model 6 - Polynomial Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffd13fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs for the Pipeline Controller\n",
    "\n",
    "drop_columns = [\"id\", \"date\",\"sqft_lot\", \"sqft_above\", \"sqft_basement\", \"sqft_lot15\", \"condition\", \"floors\", \"sqft_living15\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682ee51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Control\n",
    "\n",
    "hp_df = (load_data(\"Data_MidTerm_Project_Real_State_Regression.xls\", source_type=\"excel\")\n",
    "                   .pipe(standard_headings)\n",
    "                   .drop(drop_columns, axis=1)\n",
    "                   .pipe(transform_renovated_built)  # Combination of yr_built and yr_renovated into one single feature\n",
    "                   .pipe(remove_rows)\n",
    "                   .reset_index(drop=True)\n",
    "                   .pipe(normalizations)     # Price and sqft_living Normalization\n",
    "                   .pipe(dist_to)            # Location Enigeneered Feature\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4433d5",
   "metadata": {},
   "source": [
    "Model 6 considers a reduced set of features, the discussed transformations and a polynomial regression of order 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fc2871",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = hp_df[\"price\"]\n",
    "x = hp_df.drop([\"price\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e56235",
   "metadata": {},
   "outputs": [],
   "source": [
    "model6_transformed = my_transformations(x, y, test_size=0.2, numerical=True, scaler=None, categorical=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abf7cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.concat([model6_transformed[\"train\"][0], model6_transformed[\"train\"][1]], axis=1)\n",
    "y_train = model6_transformed[\"train\"][3]\n",
    "\n",
    "x_test = pd.concat([model6_transformed[\"test\"][0], model6_transformed[\"test\"][1]], axis=1)\n",
    "y_test = model6_transformed[\"test\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb795a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Transformation - order n\n",
    "\n",
    "n = 2\n",
    "polynomial_features= PolynomialFeatures(degree=n)\n",
    "x_train_poly = polynomial_features.fit_transform(x_train)\n",
    "x_test_poly = polynomial_features.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af214ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model6_output = regression(x_train_poly, y_train, x_test_poly, y_test, LinearRegression(), cv=10, verbose=True, plot=True, y_transformer=\"log\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f41d1d6",
   "metadata": {},
   "source": [
    "## Model 7 - XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd0d22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs for the Pipeline Controller\n",
    "\n",
    "drop_columns = [\"id\", \"date\",\"sqft_lot\", \"sqft_above\", \"sqft_basement\", \"sqft_lot15\", \"condition\", \"floors\", \"sqft_living15\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31beb5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Control\n",
    "\n",
    "hp_df = (load_data(\"Data_MidTerm_Project_Real_State_Regression.xls\", source_type=\"excel\")\n",
    "                   .pipe(standard_headings)\n",
    "                   .drop(drop_columns, axis=1)\n",
    "                   .pipe(transform_renovated_built)  # Combination of yr_built and yr_renovated into one single feature\n",
    "                   .pipe(remove_rows)\n",
    "                   .reset_index(drop=True)\n",
    "                   .pipe(normalizations)     # Price and sqft_living Normalization\n",
    "                   .pipe(dist_to)            # Location Enigeneered Feature\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebe09ea",
   "metadata": {},
   "source": [
    "Model 7 considers a reduced set of features, the discussed transformations and a Extreme Gradient Boost model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686ab432",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = hp_df[\"price\"]\n",
    "x = hp_df.drop([\"price\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7ef097",
   "metadata": {},
   "outputs": [],
   "source": [
    "model7_transformed = my_transformations(x, y, test_size=0.2, numerical=True, scaler=None, categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e39831",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.concat([model7_transformed[\"train\"][0], model7_transformed[\"train\"][1]], axis=1)\n",
    "y_train = model7_transformed[\"train\"][3]\n",
    "\n",
    "x_test = pd.concat([model7_transformed[\"test\"][0], model7_transformed[\"test\"][1]], axis=1)\n",
    "y_test = model7_transformed[\"test\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de6fa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model7_output = regression(x_train, y_train, x_test, y_test, xgboost.XGBRegressor(n_estimators=90, learning_rate=0.1, gamma=0, subsample=0.75,\n",
    "                           colsample_bytree=1, max_depth=6), cv=10, verbose=True, plot=True, y_transformer=\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9eb42e",
   "metadata": {},
   "source": [
    "# Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05568525",
   "metadata": {},
   "source": [
    "- An Extreme Grandient Boosting (XGB) model with an adjusted R2=0.9 has been developed to prediced the price of houses in Kings County Area, Seattle.\n",
    "- The most revelant estimators are the living area (sqft_living) and the building grade (quality).\n",
    "- This high performance has been obtained using only 12 estimators.\n",
    "- Normalization of the price and the living area demonstrated to be very powerful in this dataset, bringing a 4% improvement on the adjusted R2\n",
    "- The Location Engineered Features brought a 6% improvement on the adjusted R2. These feature are based on the dataset longitud and latitud data and represent the distance of each house to two hot-spots (areas with very high prices), belleveu and downtown Seattle\n",
    "- There are a lot of outliers in the data (houses with very prices). However it makes sense to keep them as they are part of the nature of the data. There are house with very high prices on the market but with very specific characteristics: huge living space, water access, very high grade, very nice locations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
